#!/usr/bin/env python3
"""
Token ç»Ÿè®¡è„šæœ¬

åˆ†æ .agent åè®®æ–‡æ¡£çš„ Token å ç”¨ï¼Œå¸®åŠ©ä¼˜åŒ–åè®®å¤§å°ã€‚

æ³¨æ„ï¼šä½¿ç”¨ç®€å•çš„ Token ä¼°ç®—æ–¹æ³•ï¼ˆå•è¯ + æ ‡ç‚¹ï¼‰ï¼Œ
å®é™… Token æ•°é‡å–å†³äºå…·ä½“çš„ tokenizerã€‚
"""

import argparse
import re
import sys
from dataclasses import dataclass
from pathlib import Path


@dataclass
class FileStats:
    """æ–‡ä»¶ç»Ÿè®¡"""
    path: str
    chars: int
    words: int
    lines: int
    tokens_estimate: int


def estimate_tokens(text: str) -> int:
    """
    ä¼°ç®— Token æ•°é‡ã€‚
    
    ç®€å•ä¼°ç®—æ–¹æ³•ï¼š
    - è‹±æ–‡ï¼šçº¦ 1 token / 4 chars
    - ä¸­æ–‡ï¼šçº¦ 1 token / 2 chars
    - ä»£ç /æ ‡ç‚¹ï¼šé¢å¤–è®¡æ•°
    
    è¿™æ˜¯ç²—ç•¥ä¼°ç®—ï¼Œå®é™…è¯·ä½¿ç”¨ tiktoken ç­‰åº“ã€‚
    """
    # åˆ†ç¦»ä¸­è‹±æ–‡
    chinese = re.findall(r'[\u4e00-\u9fff]', text)
    english_words = re.findall(r'[a-zA-Z]+', text)
    numbers = re.findall(r'\d+', text)
    punctuation = re.findall(r'[^\w\s]', text)
    
    # ä¼°ç®—
    chinese_tokens = len(chinese) * 1.5  # ä¸­æ–‡å­—ç¬¦é€šå¸¸ 1-2 tokens
    english_tokens = sum(max(1, len(w) / 4) for w in english_words)
    number_tokens = len(numbers)
    punct_tokens = len(punctuation) * 0.5
    
    return int(chinese_tokens + english_tokens + number_tokens + punct_tokens)


def analyze_file(path: Path, base_dir: Path) -> FileStats:
    """åˆ†æå•ä¸ªæ–‡ä»¶"""
    content = path.read_text(encoding="utf-8")
    
    return FileStats(
        path=str(path.relative_to(base_dir)),
        chars=len(content),
        words=len(content.split()),
        lines=content.count('\n') + 1,
        tokens_estimate=estimate_tokens(content),
    )


def analyze_directory(agent_dir: Path) -> dict[str, list[FileStats]]:
    """åˆ†æç›®å½•"""
    results: dict[str, list[FileStats]] = {}
    
    for path in agent_dir.rglob("*.md"):
        relative = path.relative_to(agent_dir)
        
        # æŒ‰é¡¶çº§ç›®å½•åˆ†ç»„
        if len(relative.parts) > 1:
            category = relative.parts[0]
        else:
            category = "root"
        
        if category not in results:
            results[category] = []
        
        results[category].append(analyze_file(path, agent_dir))
    
    return results


def format_size(size: int) -> str:
    """æ ¼å¼åŒ–å¤§å°"""
    if size < 1000:
        return str(size)
    elif size < 1000000:
        return f"{size/1000:.1f}K"
    else:
        return f"{size/1000000:.1f}M"


def main():
    parser = argparse.ArgumentParser(description="Count tokens in .agent protocol")
    parser.add_argument(
        "--agent-dir",
        default=".agent",
        help="Path to .agent directory",
    )
    parser.add_argument(
        "--format",
        choices=["text", "json", "csv"],
        default="text",
        help="Output format",
    )
    parser.add_argument(
        "--sort",
        choices=["path", "tokens", "chars"],
        default="tokens",
        help="Sort by",
    )
    parser.add_argument(
        "--top",
        type=int,
        default=0,
        help="Show only top N files",
    )
    
    args = parser.parse_args()
    
    agent_dir = Path(args.agent_dir)
    if not agent_dir.exists():
        print(f"Error: .agent directory not found at {agent_dir}")
        sys.exit(1)
    
    results = analyze_directory(agent_dir)
    
    # æ±‡æ€»ç»Ÿè®¡
    all_files: list[FileStats] = []
    for category_files in results.values():
        all_files.extend(category_files)
    
    # æ’åº
    sort_key = {
        "path": lambda x: x.path,
        "tokens": lambda x: -x.tokens_estimate,
        "chars": lambda x: -x.chars,
    }[args.sort]
    all_files.sort(key=sort_key)
    
    if args.top > 0:
        all_files = all_files[:args.top]
    
    # è¾“å‡º
    if args.format == "json":
        import json
        output = {
            "files": [vars(f) for f in all_files],
            "summary": {
                "total_files": len(all_files),
                "total_tokens": sum(f.tokens_estimate for f in all_files),
                "total_chars": sum(f.chars for f in all_files),
            }
        }
        print(json.dumps(output, indent=2))
    
    elif args.format == "csv":
        print("path,chars,words,lines,tokens_estimate")
        for f in all_files:
            print(f"{f.path},{f.chars},{f.words},{f.lines},{f.tokens_estimate}")
    
    else:
        print("=== Token Statistics ===\n")
        
        # æŒ‰ç±»åˆ«æ±‡æ€»
        print("By Category:")
        print("-" * 50)
        for category in sorted(results.keys()):
            files = results[category]
            total_tokens = sum(f.tokens_estimate for f in files)
            total_chars = sum(f.chars for f in files)
            print(f"  {category:20} {len(files):3} files  "
                  f"{format_size(total_tokens):>8} tokens  "
                  f"{format_size(total_chars):>8} chars")
        
        print("\nTop Files by Tokens:")
        print("-" * 50)
        
        display_files = all_files[:10] if args.top == 0 else all_files
        for f in display_files:
            print(f"  {f.path:40} {format_size(f.tokens_estimate):>8} tokens")
        
        # æ€»è®¡
        total_tokens = sum(f.tokens_estimate for f in all_files)
        total_chars = sum(f.chars for f in all_files)
        total_lines = sum(f.lines for f in all_files)
        
        print("\n" + "=" * 50)
        print(f"Total: {len(all_files)} files")
        print(f"  Tokens (estimated): {format_size(total_tokens)}")
        print(f"  Characters: {format_size(total_chars)}")
        print(f"  Lines: {format_size(total_lines)}")
        
        # Token é¢„ç®—å»ºè®®
        print("\nğŸ“Š Token Budget Analysis:")
        if total_tokens < 5000:
            print("  âœ“ Small protocol - suitable for single-context loading")
        elif total_tokens < 10000:
            print("  âš  Medium protocol - consider selective loading")
        else:
            print("  âŒ Large protocol - requires on-demand loading strategy")


if __name__ == "__main__":
    main()
